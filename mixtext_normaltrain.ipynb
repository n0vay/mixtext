{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "history_visible": true,
      "mount_file_id": "1DlP7uoU21VC4Bx6P5zQj_g_mWsnB3Z5f",
      "authorship_tag": "ABX9TyORh4QfNwfwnqNImwMYst+E",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/n0vay/mixtext/blob/main/mixtext_normaltrain.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/SALT-NLP/MixText.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XHAZ8bkp8OoI",
        "outputId": "c521cde1-8a24-40dc-b513-0316bc13a02e"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'MixText' already exists and is not an empty directory.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "data_dir = './drive/MyDrive/mixtext/'"
      ],
      "metadata": {
        "id": "rNDCQHDPEXJN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "521c0bdd-af90-4041-cd39-a63c7cb8bf2b"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pytorch-transformers\n",
        "from pytorch_transformers import *"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z5F5Xf0mDj8c",
        "outputId": "c4d52fbf-dab4-43b9-cb0a-bbc0f0b7f1e6"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: pytorch-transformers in /usr/local/lib/python3.7/dist-packages (1.2.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from pytorch-transformers) (4.64.1)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from pytorch-transformers) (0.0.53)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from pytorch-transformers) (2.23.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from pytorch-transformers) (1.21.6)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.7/dist-packages (from pytorch-transformers) (2022.6.2)\n",
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.7/dist-packages (from pytorch-transformers) (1.26.8)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.7/dist-packages (from pytorch-transformers) (0.1.97)\n",
            "Requirement already satisfied: torch>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from pytorch-transformers) (1.12.1+cu113)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.0.0->pytorch-transformers) (4.1.1)\n",
            "Requirement already satisfied: botocore<1.30.0,>=1.29.8 in /usr/local/lib/python3.7/dist-packages (from boto3->pytorch-transformers) (1.29.8)\n",
            "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /usr/local/lib/python3.7/dist-packages (from boto3->pytorch-transformers) (1.0.1)\n",
            "Requirement already satisfied: s3transfer<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from boto3->pytorch-transformers) (0.6.0)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.7/dist-packages (from botocore<1.30.0,>=1.29.8->boto3->pytorch-transformers) (2.8.2)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.25.4 in /usr/local/lib/python3.7/dist-packages (from botocore<1.30.0,>=1.29.8->boto3->pytorch-transformers) (1.25.11)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.30.0,>=1.29.8->boto3->pytorch-transformers) (1.15.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->pytorch-transformers) (2022.9.24)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->pytorch-transformers) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->pytorch-transformers) (3.0.4)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->pytorch-transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->pytorch-transformers) (1.2.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "32SXqwNNpwl3"
      },
      "outputs": [],
      "source": [
        "import argparse\n",
        "import os\n",
        "import random\n",
        "import math\n",
        "import pickle"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.utils.data as Data\n",
        "from torch.autograd import Variable\n",
        "from torch.utils.data import Dataset"
      ],
      "metadata": {
        "id": "ZHCx-yr_NNa1"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ClassificationBert(nn.Module):\n",
        "    def __init__(self, num_labels=2):\n",
        "        super(ClassificationBert, self).__init__()\n",
        "        # Load pre-trained bert model\n",
        "        self.bert = BertModel.from_pretrained('bert-base-uncased')\n",
        "        self.linear = nn.Sequential(nn.Linear(768, 128),\n",
        "                                    nn.Tanh(),\n",
        "                                    nn.Linear(128, num_labels))\n",
        "\n",
        "    def forward(self, x, length=256):\n",
        "        # Encode input text\n",
        "        all_hidden, pooler = self.bert(x)\n",
        "\n",
        "        pooled_output = torch.mean(all_hidden, 1)\n",
        "        # Use linear layer to do the predictions\n",
        "        predict = self.linear(pooled_output)\n",
        "\n",
        "        return predict"
      ],
      "metadata": {
        "id": "lBDvbzyuOLMy"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Read Data"
      ],
      "metadata": {
        "id": "0Df1ymVi__hu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Translator:\n",
        "    \"\"\"Backtranslation. Here to save time, we pre-processing and save all the translated data into pickle files.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, path, transform_type='BackTranslation'):\n",
        "        # Pre-processed German data\n",
        "        with open(path + 'de_1.pkl', 'rb') as f:\n",
        "            self.de = pickle.load(f)\n",
        "        # Pre-processed Russian data\n",
        "        with open(path + 'ru_1.pkl', 'rb') as f:\n",
        "            self.ru = pickle.load(f)\n",
        "\n",
        "    def __call__(self, ori, idx):\n",
        "        out1 = self.de[idx]\n",
        "        out2 = self.ru[idx]\n",
        "        return out1, out2, ori"
      ],
      "metadata": {
        "id": "AVBATLUeNUFO"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_data(data_path, n_labeled_per_class, unlabeled_per_class=5000, max_seq_len=256, model='bert-base-uncased', train_aug=False):\n",
        "    \"\"\"Read data, split the dataset, and build dataset for dataloaders.\n",
        "    Arguments:\n",
        "        data_path {str} -- Path to your dataset folder: contain a train.csv and test.csv\n",
        "        n_labeled_per_class {int} -- Number of labeled data per class\n",
        "    Keyword Arguments:\n",
        "        unlabeled_per_class {int} -- Number of unlabeled data per class (default: {5000})\n",
        "        max_seq_len {int} -- Maximum sequence length (default: {256})\n",
        "        model {str} -- Model name (default: {'bert-base-uncased'})\n",
        "        train_aug {bool} -- Whether performing augmentation on labeled training set (default: {False})\n",
        "    \"\"\"\n",
        "    # Load the tokenizer for bert\n",
        "    tokenizer = BertTokenizer.from_pretrained(model)\n",
        "\n",
        "    train_df = pd.read_csv(data_path+'train.csv', header=None)\n",
        "    test_df = pd.read_csv(data_path+'test.csv', header=None)\n",
        "\n",
        "    # Here we only use the bodies and removed titles to do the classifications\n",
        "    train_labels = np.array([v-1 for v in train_df[0]])\n",
        "    train_text = np.array([v for v in train_df[2]])\n",
        "\n",
        "    test_labels = np.array([u-1 for u in test_df[0]])\n",
        "    test_text = np.array([v for v in test_df[2]])\n",
        "\n",
        "    n_labels = max(test_labels) + 1\n",
        "\n",
        "    # Split the labeled training set, unlabeled training set, development set\n",
        "    train_labeled_idxs, train_unlabeled_idxs, val_idxs = train_val_split(\n",
        "        train_labels, n_labeled_per_class, unlabeled_per_class, n_labels)\n",
        "\n",
        "    # Build the dataset class for each set\n",
        "    train_labeled_dataset = loader_labeled(\n",
        "        train_text[train_labeled_idxs], train_labels[train_labeled_idxs], tokenizer, max_seq_len, train_aug)\n",
        "    train_unlabeled_dataset = loader_unlabeled(\n",
        "        train_text[train_unlabeled_idxs], train_unlabeled_idxs, tokenizer, max_seq_len, Translator(data_path))\n",
        "    val_dataset = loader_labeled(\n",
        "        train_text[val_idxs], train_labels[val_idxs], tokenizer, max_seq_len)\n",
        "    test_dataset = loader_labeled(\n",
        "        test_text, test_labels, tokenizer, max_seq_len)\n",
        "\n",
        "    print(\"#Labeled: {}, Unlabeled {}, Val {}, Test {}\".format(len(\n",
        "        train_labeled_idxs), len(train_unlabeled_idxs), len(val_idxs), len(test_labels)))\n",
        "\n",
        "    return train_labeled_dataset, train_unlabeled_dataset, val_dataset, test_dataset, n_labels"
      ],
      "metadata": {
        "id": "IYGTLJnjO8EC"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_val_split(labels, n_labeled_per_class, unlabeled_per_class, n_labels, seed=0):\n",
        "    \"\"\"Split the original training set into labeled training set, unlabeled training set, development set\n",
        "    Arguments:\n",
        "        labels {list} -- List of labeles for original training set\n",
        "        n_labeled_per_class {int} -- Number of labeled data per class\n",
        "        unlabeled_per_class {int} -- Number of unlabeled data per class\n",
        "        n_labels {int} -- The number of classes\n",
        "    Keyword Arguments:\n",
        "        seed {int} -- [random seed of np.shuffle] (default: {0})\n",
        "    Returns:\n",
        "        [list] -- idx for labeled training set, unlabeled training set, development set\n",
        "    \"\"\"\n",
        "    np.random.seed(seed)\n",
        "    labels = np.array(labels)\n",
        "    train_labeled_idxs = []\n",
        "    train_unlabeled_idxs = []\n",
        "    val_idxs = []\n",
        "\n",
        "    for i in range(n_labels):\n",
        "        idxs = np.where(labels == i)[0]\n",
        "        np.random.shuffle(idxs)\n",
        "        if n_labels == 2:\n",
        "            # IMDB\n",
        "            train_pool = np.concatenate((idxs[:500], idxs[5500:-2000]))\n",
        "            train_labeled_idxs.extend(train_pool[:n_labeled_per_class])\n",
        "            train_unlabeled_idxs.extend(\n",
        "                idxs[500: 500 + 5000])\n",
        "            val_idxs.extend(idxs[-2000:])\n",
        "        elif n_labels == 10:\n",
        "            # DBPedia\n",
        "            train_pool = np.concatenate((idxs[:500], idxs[10500:-2000]))\n",
        "            train_labeled_idxs.extend(train_pool[:n_labeled_per_class])\n",
        "            train_unlabeled_idxs.extend(\n",
        "                idxs[500: 500 + unlabeled_per_class])\n",
        "            val_idxs.extend(idxs[-2000:])\n",
        "        else:\n",
        "            # Yahoo/AG News\n",
        "            train_pool = np.concatenate((idxs[:500], idxs[5500:-2000]))\n",
        "            train_labeled_idxs.extend(train_pool[:n_labeled_per_class])\n",
        "            train_unlabeled_idxs.extend(\n",
        "                idxs[500: 500 + 5000])\n",
        "            val_idxs.extend(idxs[-2000:])\n",
        "    np.random.shuffle(train_labeled_idxs)\n",
        "    np.random.shuffle(train_unlabeled_idxs)\n",
        "    np.random.shuffle(val_idxs)\n",
        "\n",
        "    return train_labeled_idxs, train_unlabeled_idxs, val_idxs"
      ],
      "metadata": {
        "id": "YutXZervAb3R"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class loader_labeled(Dataset):\n",
        "    # Data loader for labeled data\n",
        "    def __init__(self, dataset_text, dataset_label, tokenizer, max_seq_len, aug=False):\n",
        "        self.tokenizer = tokenizer\n",
        "        self.text = dataset_text\n",
        "        self.labels = dataset_label\n",
        "        self.max_seq_len = max_seq_len\n",
        "\n",
        "        self.aug = aug\n",
        "        self.trans_dist = {}\n",
        "\n",
        "        if aug:\n",
        "            print('Aug train data by back translation of German')\n",
        "            self.en2de = torch.hub.load(\n",
        "                'pytorch/fairseq', 'transformer.wmt19.en-de.single_model', tokenizer='moses', bpe='fastbpe')\n",
        "            self.de2en = torch.hub.load(\n",
        "                'pytorch/fairseq', 'transformer.wmt19.de-en.single_model', tokenizer='moses', bpe='fastbpe')\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "    def augment(self, text):\n",
        "        if text not in self.trans_dist:\n",
        "            self.trans_dist[text] = self.de2en.translate(self.en2de.translate(\n",
        "                text,  sampling=True, temperature=0.9),  sampling=True, temperature=0.9)\n",
        "        return self.trans_dist[text]\n",
        "\n",
        "    def get_tokenized(self, text):\n",
        "        tokens = self.tokenizer.tokenize(text)\n",
        "        if len(tokens) > self.max_seq_len:\n",
        "            tokens = tokens[:self.max_seq_len]\n",
        "        length = len(tokens)\n",
        "\n",
        "        encode_result = self.tokenizer.convert_tokens_to_ids(tokens)\n",
        "        padding = [0] * (self.max_seq_len - len(encode_result))\n",
        "        encode_result += padding\n",
        "\n",
        "        return encode_result, length\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        if self.aug:\n",
        "            text = self.text[idx]\n",
        "            text_aug = self.augment(text)\n",
        "            text_result, text_length = self.get_tokenized(text)\n",
        "            text_result2, text_length2 = self.get_tokenized(text_aug)\n",
        "            return ((torch.tensor(text_result), torch.tensor(text_result2)), (self.labels[idx], self.labels[idx]), (text_length, text_length2))\n",
        "        else:\n",
        "            text = self.text[idx]\n",
        "            tokens = self.tokenizer.tokenize(text)\n",
        "            if len(tokens) > self.max_seq_len:\n",
        "                tokens = tokens[:self.max_seq_len]\n",
        "            length = len(tokens)\n",
        "            encode_result = self.tokenizer.convert_tokens_to_ids(tokens)\n",
        "            padding = [0] * (self.max_seq_len - len(encode_result))\n",
        "            encode_result += padding\n",
        "            return (torch.tensor(encode_result), self.labels[idx], length)"
      ],
      "metadata": {
        "id": "lXOCInGCAopl"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class loader_unlabeled(Dataset):\n",
        "    # Data loader for unlabeled data\n",
        "    def __init__(self, dataset_text, unlabeled_idxs, tokenizer, max_seq_len, aug=None):\n",
        "        self.tokenizer = tokenizer\n",
        "        self.text = dataset_text\n",
        "        self.ids = unlabeled_idxs\n",
        "        self.aug = aug\n",
        "        self.max_seq_len = max_seq_len\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.text)\n",
        "\n",
        "    def get_tokenized(self, text):\n",
        "        tokens = self.tokenizer.tokenize(text)\n",
        "        if len(tokens) > self.max_seq_len:\n",
        "            tokens = tokens[:self.max_seq_len]\n",
        "        length = len(tokens)\n",
        "        encode_result = self.tokenizer.convert_tokens_to_ids(tokens)\n",
        "        padding = [0] * (self.max_seq_len - len(encode_result))\n",
        "        encode_result += padding\n",
        "        return encode_result, length\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        if self.aug is not None:\n",
        "            u, v, ori = self.aug(self.text[idx], self.ids[idx])\n",
        "            encode_result_u, length_u = self.get_tokenized(u)\n",
        "            encode_result_v, length_v = self.get_tokenized(v)\n",
        "            encode_result_ori, length_ori = self.get_tokenized(ori)\n",
        "            return ((torch.tensor(encode_result_u), torch.tensor(encode_result_v), torch.tensor(encode_result_ori)), (length_u, length_v, length_ori))\n",
        "        else:\n",
        "            text = self.text[idx]\n",
        "            encode_result, length = self.get_tokenized(text)\n",
        "            return (torch.tensor(encode_result), length)"
      ],
      "metadata": {
        "id": "p735Z0HCA6qs"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Classification Bert from Normal Bert"
      ],
      "metadata": {
        "id": "9ipWpkKEBNEy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ClassificationBert(nn.Module):\n",
        "    def __init__(self, num_labels=2):\n",
        "        super(ClassificationBert, self).__init__()\n",
        "        # Load pre-trained bert model\n",
        "        self.bert = BertModel.from_pretrained('bert-base-uncased')\n",
        "        self.linear = nn.Sequential(nn.Linear(768, 128),\n",
        "                                    nn.Tanh(),\n",
        "                                    nn.Linear(128, num_labels))\n",
        "\n",
        "    def forward(self, x, length=256):\n",
        "        # Encode input text\n",
        "        all_hidden, pooler = self.bert(x)\n",
        "\n",
        "        pooled_output = torch.mean(all_hidden, 1)\n",
        "        # Use linear layer to do the predictions\n",
        "        predict = self.linear(pooled_output)\n",
        "\n",
        "        return predict"
      ],
      "metadata": {
        "id": "HwSTvbPmBShs"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Normal Train"
      ],
      "metadata": {
        "id": "qbJUD8z7BYv7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "parser = argparse.ArgumentParser(description='PyTorch Base Models')\n",
        "\n",
        "parser.add_argument('--epochs', default=50, type=int, metavar='N',\n",
        "                    help='number of total epochs to run')\n",
        "parser.add_argument('--batch-size', default=4, type=int, metavar='N',\n",
        "                    help='train batchsize')\n",
        "parser.add_argument('--batch-size-u', default=24, type=int, metavar='N',\n",
        "                    help='train batchsize')\n",
        "\n",
        "parser.add_argument('--lrmain', '--learning-rate-bert', default=0.00001, type=float,\n",
        "                    metavar='LR', help='initial learning rate for bert')\n",
        "parser.add_argument('--lrlast', '--learning-rate-model', default=0.001, type=float,\n",
        "                    metavar='LR', help='initial learning rate for models')\n",
        "\n",
        "parser.add_argument('--gpu', default='0,1,2,3', type=str,\n",
        "                    help='id(s) for CUDA_VISIBLE_DEVICES')\n",
        "\n",
        "parser.add_argument('--n-labeled', type=int, default=20,\n",
        "                    help='Number of labeled data')\n",
        "parser.add_argument('--val-iteration', type=int, default=200,\n",
        "                    help='Number of labeled data')\n",
        "\n",
        "\n",
        "parser.add_argument('--mix-option', default=False, type=bool, metavar='N',\n",
        "                    help='mix option')\n",
        "parser.add_argument('--train_aug', default=False, type=bool, metavar='N',\n",
        "                    help='aug for training data')\n",
        "\n",
        "\n",
        "parser.add_argument('--model', type=str, default='bert-base-uncased',\n",
        "                    help='pretrained model')\n",
        "\n",
        "parser.add_argument('--data-path', type=str, default='yahoo_answers_csv/',\n",
        "                    help='path to data folders')\n",
        "\n",
        "#args = parser.parse_args()"
      ],
      "metadata": {
        "id": "D9SSBF9lBbK3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import namedtuple\n",
        "\n",
        "Arguments = namedtuple(\"Arguments\",\n",
        "    [\n",
        "    'epochs',\n",
        "    'batch_size',\n",
        "    'batch_size_u',\n",
        "    'lrmain',\n",
        "    'lrlast',\n",
        "    'gpu',\n",
        "    'n_labeled',\n",
        "    'val_iteration',\n",
        "    'mix_option',\n",
        "    'train_aug',\n",
        "    'model',\n",
        "    \"data_path\"\n",
        "     ])\n",
        "\n",
        "args = Arguments(\n",
        "    5,\n",
        "    2,\n",
        "    4,\n",
        "    0.00001,\n",
        "    0.001,\n",
        "    '0,1,2,3',\n",
        "    20,\n",
        "    200,\n",
        "    False,\n",
        "    False,\n",
        "    'bert-base-uncased',\n",
        "    '/content/drive/MyDrive/mixtext/data/'\n",
        ")"
      ],
      "metadata": {
        "id": "0yW4PQa4Hwqa"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "os.environ['CUDA_VISIBLE_DEVICES'] = args.gpu\n",
        "use_cuda = torch.cuda.is_available()\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "n_gpu = torch.cuda.device_count()\n",
        "print(\"gpu num: \", n_gpu)\n",
        "\n",
        "best_acc = 0"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n6M3Fp7hQ7L7",
        "outputId": "e32bfbdf-8532-4f85-8433-1b18f9254d76"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "gpu num:  0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def validate(valloader, model, criterion, epoch, mode):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        loss_total = 0\n",
        "        total_sample = 0\n",
        "        acc_total = 0\n",
        "        correct = 0\n",
        "\n",
        "        for batch_idx, (inputs, targets, length) in enumerate(valloader):\n",
        "            inputs, targets = inputs.cuda(), targets.cuda(non_blocking=True)\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, targets)\n",
        "\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "\n",
        "            correct += (np.array(predicted.cpu()) ==\n",
        "                        np.array(targets.cpu())).sum()\n",
        "            loss_total += loss.item() * inputs.shape[0]\n",
        "            total_sample += inputs.shape[0]\n",
        "\n",
        "        acc_total = correct/total_sample\n",
        "        loss_total = loss_total/total_sample\n",
        "\n",
        "    return loss_total, acc_total"
      ],
      "metadata": {
        "id": "9BAUcZjIOkPr"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(labeled_trainloader, model, optimizer, criterion, epoch):\n",
        "    model.train()\n",
        "\n",
        "    for batch_idx, (inputs, targets, length) in enumerate(labeled_trainloader):\n",
        "        inputs, targets = inputs.cuda(), targets.cuda(non_blocking=True)\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, targets)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        print('epoch {}, step {}, loss {}'.format(\n",
        "            epoch, batch_idx, loss.item()))\n",
        "        loss.backward()\n",
        "        optimizer.step()"
      ],
      "metadata": {
        "id": "JnZAvfuIOoQk"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "best_acc = 0\n",
        "train_labeled_set, train_unlabeled_set, val_set, test_set, n_labels = get_data(\n",
        "    args.data_path, args.n_labeled)\n",
        "labeled_trainloader = Data.DataLoader(\n",
        "    dataset=train_labeled_set, batch_size=args.batch_size, shuffle=True)\n",
        "val_loader = Data.DataLoader(\n",
        "    dataset=val_set, batch_size=1, shuffle=False) #default 512\n",
        "test_loader = Data.DataLoader(\n",
        "    dataset=test_set, batch_size=1, shuffle=False) # default 512\n",
        "\n",
        "\n",
        "model = ClassificationBert(n_labels).cuda()\n",
        "model = nn.DataParallel(model)\n",
        "optimizer = AdamW(\n",
        "    [\n",
        "        {\"params\": model.module.bert.parameters(), \"lr\": args.lrmain},\n",
        "        {\"params\": model.module.linear.parameters(), \"lr\": args.lrlast},\n",
        "    ])\n",
        "\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "test_accs = []\n",
        "\n",
        "for epoch in range(args.epochs):\n",
        "    train(labeled_trainloader, model, optimizer, criterion, epoch)\n",
        "\n",
        "    val_loss, val_acc = validate(\n",
        "        val_loader, model, criterion, epoch, mode='Valid Stats')\n",
        "    print(\"epoch {}, val acc {}, val_loss {}\".format(\n",
        "        epoch, val_acc, val_loss))\n",
        "\n",
        "    if val_acc >= best_acc:\n",
        "        best_acc = val_acc\n",
        "        test_loss, test_acc = validate(\n",
        "            test_loader, model, criterion, epoch, mode='Test Stats ')\n",
        "        test_accs.append(test_acc)\n",
        "        print(\"epoch {}, test acc {},test loss {}\".format(\n",
        "            epoch, test_acc, test_loss))\n",
        "\n",
        "print('Best val_acc:')\n",
        "print(best_acc)\n",
        "\n",
        "print('Test acc:')\n",
        "print(test_accs)"
      ],
      "metadata": {
        "id": "VmM2xmswNgvU"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}